{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67820cb",
   "metadata": {},
   "source": [
    "# Building MicroGrad from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759456a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa59eb",
   "metadata": {},
   "source": [
    "## Core value  object of micrograd and it visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "#the __init__ method to initialize the object\n",
    "    def __init__(self, data,_children=(), _op='',label=\"\"): \n",
    "        \n",
    "        self.data= data\n",
    "        #initially assume that will be 0 means no effect\n",
    "        self.grad=0.0 \n",
    "        self._prev=set(_children)\n",
    "        self._op = _op\n",
    "        self.label=label\n",
    "        \n",
    "        \n",
    "#special method used to represent a class's objects as a string       \n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self,other): \n",
    "        out= Value(self.data + other.data,(self,other),'+')\n",
    "        #(self,other) is _children\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        out= Value(self.data*other.data, (self,other),'*')\n",
    "        #(self,other) is _children\n",
    "        return out\n",
    "    \n",
    "    #created for Manual backpropagation example 2 (a neuron)\n",
    "    def tanh(self): \n",
    "        x=self.data\n",
    "        #Hyperpolic tangent exponential function\n",
    "        t=(math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "        out=Value(t, (self, ),'tanh') #(self, ) means just one child\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "a=Value(2.0,label=\"a\")\n",
    "b=Value(-3.0,label=\"b\")\n",
    "c=Value(10.0,label=\"c\")\n",
    "e=a*b; e.label=\"e\"\n",
    "d=e+c ;d.label=\"d\"\n",
    "f=Value(-2.0,label='f')\n",
    "L=d*f ; L.label=\"L\"\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264f772",
   "metadata": {},
   "source": [
    "**grad**\n",
    "\n",
    "take note that ``self.grad`` is the derivative of a function that has more than one input variable (exp:dL/df).\n",
    "\n",
    "The gradient simply measures the change in all weights with regard to the change in error.\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "``set()``\n",
    "Set items are **unordered**, **unchangeable**, and **do not allow duplicate values**.\n",
    "\n",
    "**unordered** - items in a set do not have a defined order.\n",
    "\n",
    "**unchangeable** - cannot change the items after the set has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a7b00",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2991b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "        for child in v._prev:\n",
    "            edges.add((child, v))\n",
    "            build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "    \n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "          # if this value is a result of some operation, create an op node for it\n",
    "          dot.node(name = uid + n._op, label = n._op)\n",
    "          # and connect this node to it\n",
    "          dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698bd53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26e56b",
   "metadata": {},
   "source": [
    "## Manual Backpropagation example 1 (simple expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7662e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Gradient Check\n",
    "def lol():\n",
    "    \n",
    "    h=0.0001\n",
    "    a=Value(2.0,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L1=L.data \n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    a=Value(2.0 + h ,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L2=L.data\n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol() #L with respect to a becoz we bumped a little bit by h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ead217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we manually let L.grad=1\n",
    "L.grad=1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cafd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa219fd",
   "metadata": {},
   "source": [
    "Basically we have \n",
    "\n",
    "L=d*f\n",
    "\n",
    "we would like to know\n",
    "\n",
    "dL/dd= ?\n",
    "\n",
    "it will be dL/dd= f\n",
    "\n",
    "\n",
    "**definiton of derivative**\n",
    "\n",
    "**(f(x+h)-f(x))/h**\n",
    "\n",
    "**proof**\n",
    "((d+h)*f - d*f)/h\n",
    "\n",
    "=(d*f+h*f -d*f)/h\n",
    "\n",
    "=h*f/h\n",
    "\n",
    "= f\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab02c8",
   "metadata": {},
   "source": [
    "**L=d*f**\n",
    "\n",
    "According to the visual above, we know that \n",
    "\n",
    "dL/df= d\n",
    "\n",
    "then\n",
    "\n",
    "f.grad =4.0 \n",
    "\n",
    "\n",
    "dL/dd= f\n",
    "\n",
    "then\n",
    "\n",
    "d.grad=-2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.grad =4.0 \n",
    "d.grad=-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4380b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the result will be\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the answer f.grad is correct or not?\n",
    "\n",
    "#Gradient Check\n",
    "def lol():\n",
    "    \n",
    "    h=0.0001\n",
    "    a=Value(2.0,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L1=L.data \n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    a=Value(2.0 ,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0 +h,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L2=L.data\n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol() #L with respect to f becoz we bumped a little bit by h\n",
    "\n",
    "#it just f.grad or dL/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the answer d.grad is correct or not?\n",
    "\n",
    "#Gradient Check\n",
    "\n",
    "def lol():\n",
    "    \n",
    "    h=0.0001\n",
    "    \n",
    "    a=Value(2.0,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L1=L.data \n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    a=Value(2.0 ,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    d.data+=h\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L2=L.data\n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol() #L with respect to d becoz we bumped a little bit by h\n",
    "\n",
    "#it just d.grad or dL/dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36879914",
   "metadata": {},
   "source": [
    "Next we need to derive **dL/dc**\n",
    "\n",
    "dd/dc ?\n",
    "\n",
    "d=c+e\n",
    "\n",
    "dd/dc=1\n",
    "\n",
    "**definiton of derivative**\n",
    "\n",
    "**(f(x+h)-f(x))/h**\n",
    "\n",
    "**Proof for dd/dc=1**\n",
    "( ( (c+h)+e )-(c+e) )/h\n",
    "\n",
    "=(c+h+e-c-e)/h\n",
    "\n",
    "=h/h\n",
    "\n",
    "=1\n",
    "\n",
    "dd/de= 1 too.\n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "take note that dd/dc and dd/de are the infomation **how c and e impact d**, which are local gradient\n",
    "\n",
    "and our **goal is to know how e and c impact L.**\n",
    "\n",
    "To find **dL/dc** and **dL/de**, use **chain rule**\n",
    "\n",
    "dL/dc= (dl/dd)*(dd/dc) = -2 *1= -2\n",
    "\n",
    "dL/de=(dl/dd)*(dd/de)=-2 *1 = -2 \n",
    "\n",
    "\n",
    "Hence \n",
    "\n",
    "c.grad=-2.0\n",
    "e.grad = -2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14687a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.grad=-2.0\n",
    "e.grad = -2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the result will be\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Check\n",
    "\n",
    "def lol():\n",
    "    \n",
    "    h=0.0001\n",
    "    \n",
    "    a=Value(2.0,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L1=L.data \n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    a=Value(2.0 ,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    c.data+=h\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L2=L.data\n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol() #L with respect to c becoz we bumped a little bit by h\n",
    "\n",
    "#it just c.grad or dL/dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f64eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Check\n",
    "\n",
    "def lol():\n",
    "    \n",
    "    h=0.0001\n",
    "    \n",
    "    a=Value(2.0,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L1=L.data \n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    a=Value(2.0 ,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    e.data+=h\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L2=L.data\n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol() #L with respect to e becoz we bumped a little bit by h\n",
    "\n",
    "#it just e.grad or dL/de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f7b9c",
   "metadata": {},
   "source": [
    "Next we need to derive **dL/da** and  **dL/db**\n",
    "\n",
    "we known that\n",
    "\n",
    "dL/de=-2.0\n",
    "\n",
    "then \n",
    "\n",
    "dL/da=(dL/de)*(de/da)\n",
    "\n",
    "dL/db=(dL/de)*(de/db)\n",
    "\n",
    "\n",
    "\n",
    "We need To find local gradient which de/da\n",
    "\n",
    "e=a*b\n",
    "\n",
    "de/da=b = -3\n",
    "de/db=a = 2\n",
    "\n",
    "then\n",
    "\n",
    "dL/da=(dL/de)*(de/da)= -2 * -3 = 6\n",
    "\n",
    "dL/db=(dL/de)*(de/db)= -2 * 2 = -4\n",
    "\n",
    "Hence\n",
    "\n",
    "a.grad=6\n",
    "b.grad=-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12747ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad=6.0\n",
    "b.grad=-4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa91357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the result will be\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Check\n",
    "\n",
    "def lol():\n",
    "    \n",
    "    h=0.001\n",
    "    \n",
    "    a=Value(2.0,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L1=L.data \n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    a=Value(2.0 ,label=\"a\")\n",
    "    a.data+=h\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L2=L.data\n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol() #L with respect to a becoz we bumped a little bit by h\n",
    "\n",
    "#it just a.grad or dL/da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Check\n",
    "\n",
    "def lol():\n",
    "    \n",
    "    h=0.001\n",
    "    \n",
    "    a=Value(2.0,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L1=L.data \n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    a=Value(2.0 ,label=\"a\")\n",
    "    b=Value(-3.0,label=\"b\")\n",
    "    b.data+=h\n",
    "    c=Value(10.0,label=\"c\")\n",
    "    e=a*b; e.label=\"e\"\n",
    "    d=e+c ;d.label=\"d\"\n",
    "    f=Value(-2.0,label='f')\n",
    "    L=d*f ; L.label=\"L\"\n",
    "    L2=L.data\n",
    "    #careful that l is value node, and we want its data, then L.data\n",
    "    \n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol() #L with respect to b becoz we bumped a little bit by h\n",
    "\n",
    "#it just b.grad or dL/db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45593f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final result\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6345fd",
   "metadata": {},
   "source": [
    "We **iterated through all of the nodes one by one, applying the chain rule locally**.\n",
    "\n",
    "We can determine the **derivative of L with respect to the variables (a,b,c....,f)**. We have pointers to the children nodes and perform some operation on them to find output (the derivative of L with respect to the variables). These operations allow us to determine the local derivatives.\n",
    "\n",
    "In fact, **the operations mean that we just go through and recursively multiply on the local derivative. It is just recursive application of chain rule backwards through the computation graph.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3768eb",
   "metadata": {},
   "source": [
    "##  Preview of single optimization step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe90b3",
   "metadata": {},
   "source": [
    "If we want to increase L, we should increase the data in the direction of gradient by small step amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.data += 0.01 * a.grad #step size\n",
    "b.data += 0.01 * b.grad\n",
    "c.data += 0.01 * c.grad\n",
    "f.data += 0.01 * f.grad\n",
    "\n",
    "\n",
    "e=a*b; \n",
    "d=e+c \n",
    "L=d*f ; L.label=\"L\"\n",
    "\n",
    "print(L.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3fbcc3",
   "metadata": {},
   "source": [
    "In the above example, we expect the value will be less negative.\n",
    "\n",
    "It tell us that the gradient really give us some power because we know how to influence the final outcome.\n",
    "\n",
    "It will be extremely useful for training knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dfdc10",
   "metadata": {},
   "source": [
    " ## Manual backpropagation example 2 (a neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(\"photo/neuron_model.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbfebd",
   "metadata": {},
   "source": [
    "**Convolutional Neural network for visual Recognition**\n",
    "\n",
    "We have same input (axon), then have synapse that have weigh on them (axon).\n",
    "\n",
    "The synapse interact with the input to this neuron multiplicatively (WX).\n",
    "\n",
    "Since we have many W times X flowing into the cellbody, and then the cell body have some bias.\n",
    "\n",
    "Hence the equation will be $$\\sum_{i} w_i x_i +b$$.\n",
    "\n",
    "Thus, take it through an activation function. Activiaton function is some kind of squashing function (tanh).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of activation function (squashing function)\n",
    "\n",
    "plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "# 2d neuron\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights of the neurons w1,w2\n",
    "#synaptic strengths for each input\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "#activation function output  \n",
    "o = n.tanh();o.label = 'o'\n",
    "\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6849b4e",
   "metadata": {},
   "source": [
    "Since we cannot make ``tanh`` out of just ``+`` and ``*``. This is becuase it is hyperbolic function exponentiation involve.\n",
    "\n",
    "Then we will need to define **exponentiation function** in the ``class Value``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24169353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let o.grad=1\n",
    "o.grad=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d0ea4",
   "metadata": {},
   "source": [
    "o =tahnh(n)\n",
    "\n",
    "do/dn = 1-tanh^2(n) = 1-o**2\n",
    "\n",
    "Then n.grad=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.grad=round(1-o.data**2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679466ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fa452",
   "metadata": {},
   "source": [
    "Based on previous example, a plus just a distributor of gradient. The gradint will simply flow equally, because the ``x1w1x2w2`` local derivative is 1.\n",
    "\n",
    "Then ``x1w1x2w2.grad=1``\n",
    "\n",
    "Same happen to the ``x2*w2`` and ``x1*w1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1w1x2w2.grad=0.5\n",
    "b.grad=0.5\n",
    "\n",
    "x1w1.grad=0.5\n",
    "x2w2.grad=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d804c67",
   "metadata": {},
   "source": [
    "do/dw2 = do/dx2w2 * dx2w1/dw2 = x2w2.grad * x2.data\n",
    "\n",
    "same for other\n",
    "\n",
    "do/dx2= x2w2.grad * w2.data \n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b017884",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.grad= x1w1.grad * x1.data\n",
    "x1.grad= x1w1.grad * w1.data\n",
    "\n",
    "w2.grad= x2w2.grad * x2.data\n",
    "x2.grad= x2w2.grad * w2.data\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc497722",
   "metadata": {},
   "source": [
    "If we want this neuron output to increase :\n",
    "\n",
    "W2 have no gradient, then does not matter to the neuron \n",
    "\n",
    "w1, the gradient of w1 should be go up , then the neuron output will go up ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e8a7c",
   "metadata": {},
   "source": [
    "## Implementing the backward function for each operations\n",
    "\n",
    "no more manual backpogaration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the backward function for each operations\n",
    "\n",
    "class Value:\n",
    "#the __init__ method to initialize the object\n",
    "    def __init__(self, data,_children=(), _op='',label=\"\"): \n",
    "        \n",
    "        self.data= data\n",
    "        #initially assume that will be 0 means no effect\n",
    "        self.grad=0.0 \n",
    "        #backward function \n",
    "        self._backward= lambda :None #empty function\n",
    "        self._prev=set(_children)\n",
    "        self._op = _op\n",
    "        self.label=label\n",
    "        \n",
    "        \n",
    "#special method used to represent a class's objects as a string       \n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self,other): \n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data + other.data,(self,other),'+')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            # if addition, it will 1.0\n",
    "            self.grad= 1.0* out.grad \n",
    "            other.grad =1.0* out.grad \n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data*other.data, (self,other),'*')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            \n",
    "            self.grad=other.data * out.grad\n",
    "            other.grad=self.data * out.grad \n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    #created for Manual backpropagation example 2 (a neuron)\n",
    "    def tanh(self): \n",
    "        x=self.data\n",
    "        #Hyperpolic tangent exponential function\n",
    "        t=(math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "        out=Value(t, (self, ),'tanh') #(self, ) means just one child\n",
    "\n",
    "        def _backward() :\n",
    "            self.grad =(1-t**2) * out.grad \n",
    "\n",
    "\n",
    "        out._backward=_backward\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a548db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "# 2d neuron\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights of the neurons w1,w2\n",
    "#synaptic strengths for each input\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "#activation function output  \n",
    "o = n.tanh();o.label = 'o'\n",
    "\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577b5a0",
   "metadata": {},
   "source": [
    "Now, we dont have to the backpogaration manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To initialize with 1\n",
    "o.grad= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaaa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "o._backward()\n",
    "\n",
    "n._backward()\n",
    "b._backward() # b is a leaf node, nothing will be happen (empty function)\n",
    "\n",
    "x1w1x2w2._backward()\n",
    "\n",
    "x2w2._backward()\n",
    "x1w1._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f402bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eed65e",
   "metadata": {},
   "source": [
    "## Implementing the backward function for a whole expressive graph\n",
    "\n",
    "Topological sort \n",
    "\n",
    "![topology sort](https://github.com/soonkienyuan/NOTE-build-from-scratch-neural-networks-Micrograd/blob/main/photo/1_uMg_ojFXts2WZSjcZe4oRQ.png?raw=true=10x10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the backward function for each operations\n",
    "\n",
    "class Value:\n",
    "#the __init__ method to initialize the object\n",
    "    def __init__(self, data,_children=(), _op='',label=\"\"): \n",
    "        \n",
    "        self.data= data\n",
    "        #initially assume that will be 0 means no effect\n",
    "        self.grad=0.0 \n",
    "        #backward function \n",
    "        self._backward= lambda :None #empty function\n",
    "        self._prev=set(_children)\n",
    "        self._op = _op\n",
    "        self.label=label\n",
    "        \n",
    "        \n",
    "#special method used to represent a class's objects as a string       \n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self,other): \n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data + other.data,(self,other),'+')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            # if addition, it will 1.0\n",
    "            self.grad= 1.0* out.grad \n",
    "            other.grad =1.0* out.grad \n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data*other.data, (self,other),'*')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            \n",
    "            self.grad=other.data * out.grad\n",
    "            other.grad=self.data * out.grad \n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    #backward function\n",
    "    def backward(self):\n",
    "        #topology sort\n",
    "        topo =[]\n",
    "        visited=set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad=1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "        \n",
    "    \n",
    "    #created for Manual backpropagation example 2 (a neuron)\n",
    "    def tanh(self): \n",
    "        x=self.data\n",
    "        #Hyperpolic tangent exponential function\n",
    "        t=(math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "        out=Value(t, (self, ),'tanh') #(self, ) means just one child\n",
    "\n",
    "        def _backward() :\n",
    "            self.grad =(1-t**2) * out.grad \n",
    "\n",
    "\n",
    "        out._backward=_backward\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "# 2d neuron\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights of the neurons w1,w2\n",
    "#synaptic strengths for each input\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "#activation function output  \n",
    "o = n.tanh();o.label = 'o'\n",
    "\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19842f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0f5a1",
   "metadata": {},
   "source": [
    "## Fixing backpropagation bug when one node is used multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Value(3.0,label=\"a\")\n",
    "b=a+a ; b.label=\"b\"\n",
    "\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1fa9e",
   "metadata": {},
   "source": [
    "we can see that the forward pass worked. 3+3 is 6\n",
    "\n",
    "But the gradient is wrong,  should be db/da=1+1=2\n",
    "\n",
    "**Basically, we are going to see an issue when we use a variable more than once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85334507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the backward function for each operations\n",
    "\n",
    "class Value:\n",
    "#the __init__ method to initialize the object\n",
    "    def __init__(self, data,_children=(), _op='',label=\"\"): \n",
    "        \n",
    "        self.data= data\n",
    "        #initially assume that will be 0 means no effect\n",
    "        self.grad=0.0 \n",
    "        #backward function \n",
    "        self._backward= lambda :None #empty function\n",
    "        self._prev=set(_children)\n",
    "        self._op = _op\n",
    "        self.label=label\n",
    "        \n",
    "        \n",
    "#special method used to represent a class's objects as a string       \n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self,other): \n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data + other.data,(self,other),'+')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            # if addition, it will 1.0\n",
    "            self.grad+= 1.0* out.grad # fix for the error change from + to +=\n",
    "            other.grad +=1.0* out.grad # fix for the error change from + to +=\n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data*other.data, (self,other),'*')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            \n",
    "            self.grad+=other.data * out.grad # fix for the error change from + to +=\n",
    "            other.grad+=self.data * out.grad # fix for the error change from + to +=\n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    #backward function\n",
    "    def backward(self):\n",
    "        #topology sort\n",
    "        topo =[]\n",
    "        visited=set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad=1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "        \n",
    "    \n",
    "    #created for Manual backpropagation example 2 (a neuron)\n",
    "    def tanh(self): \n",
    "        x=self.data\n",
    "        #Hyperpolic tangent exponential function\n",
    "        t=(math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "        out=Value(t, (self, ),'tanh') #(self, ) means just one child\n",
    "\n",
    "        def _backward() :\n",
    "            self.grad +=(1-t**2) * out.grad # fix for the error change from + to +=\n",
    "\n",
    "\n",
    "        out._backward=_backward\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Value(3.0,label=\"a\")\n",
    "b=a+a ; b.label=\"b\"\n",
    "\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da26d5",
   "metadata": {},
   "source": [
    "## Breaking up a tanh, exercising with more operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955634a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Value(2.0)\n",
    "#a+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e2691",
   "metadata": {},
   "source": [
    "Since 1 is not the ``Value`` object. the fix will add ``other = other if isinstance(other,Value) else Value(other)`` in __add__(self,other) and __mul__(self,other):\n",
    "\n",
    "Basically \n",
    "\n",
    "a=Value(2.0)\n",
    "a*2 is\n",
    "a.__mul__(2)\n",
    "\n",
    "when \n",
    "2*a will be error\n",
    "\n",
    "The solution is:\n",
    "\n",
    "__rmul__\n",
    "--------------------------------------------------------------\n",
    " exp(self): added for exponentiation where d/dx(tanh) will be exponential function\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Now we need add the division, power rule and substract\n",
    "\n",
    "Take note that\n",
    "a/b = a*(1/b) = a*(b**-1)\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the backward function for each operations\n",
    "\n",
    "class Value:\n",
    "#the __init__ method to initialize the object\n",
    "    def __init__(self, data,_children=(), _op='',label=\"\"): \n",
    "        \n",
    "        self.data= data\n",
    "        #initially assume that will be 0 means no effect\n",
    "        self.grad=0.0 \n",
    "        #backward function \n",
    "        self._backward= lambda :None #empty function\n",
    "        self._prev=set(_children)\n",
    "        self._op = _op\n",
    "        self.label=label\n",
    "        \n",
    "        \n",
    "#special method used to represent a class's objects as a string       \n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self,other): \n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data + other.data,(self,other),'+')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            # if addition, it will 1.0\n",
    "            self.grad+= 1.0* out.grad # fix for the error change from + to +=\n",
    "            other.grad +=1.0* out.grad # fix for the error change from + to +=\n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        #(self,other) is _children\n",
    "        out= Value(self.data*other.data, (self,other),'*')\n",
    "\n",
    "        # define the function that propagates the gradient\n",
    "        def _backward():\n",
    "            \n",
    "            self.grad+=other.data * out.grad # fix for the error change from + to +=\n",
    "            other.grad+=self.data * out.grad # fix for the error change from + to +=\n",
    "\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    #power rule (derivative rules)\n",
    "    def __pow__(self,other):\n",
    "        assert isinstance(other,(int,float)), \"only supporting int/float power for now\"\n",
    "        out=Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other -1)) * out.grad\n",
    "\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self,other): # other * self\n",
    "        return self*other\n",
    "\n",
    "\n",
    "    def __truediv__(self, other): #self /other\n",
    "        return self * other**-1 \n",
    "\n",
    "    def __neg__(self): #-self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self,other): # self - other\n",
    "        return self+ (-other)\n",
    "\n",
    "    \n",
    "    #created for Manual backpropagation example 2 (a neuron)\n",
    "    def tanh(self): \n",
    "        x=self.data\n",
    "        #Hyperpolic tangent exponential function\n",
    "        t=(math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "        out=Value(t, (self, ),'tanh') #(self, ) means just one child\n",
    "\n",
    "        def _backward() :\n",
    "            self.grad +=(1-t**2) * out.grad # fix for the error change from + to +=\n",
    "        \n",
    "        out._backward=_backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x=self.data\n",
    "        out=Value(math.exp(x),(self,),\"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad+= out.data *out.grad\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    #backward function\n",
    "    def backward(self):\n",
    "        #topology sort\n",
    "        topo =[]\n",
    "        visited=set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad=1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Value(2.0)\n",
    "b=Value(4.0)\n",
    "print(a/b) # now it works\n",
    "print(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "# 2d neuron\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights of the neurons w1,w2\n",
    "#synaptic strengths for each input\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "#---------------\n",
    "#activation function output  \n",
    "e = (2*n).exp()\n",
    "o= (e-1)/(e+1)\n",
    "\n",
    "o.label = 'o'\n",
    "#---------------\n",
    "\n",
    "o.backward()\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41192ee8",
   "metadata": {},
   "source": [
    "## Doing the same thing but in PyTorch : Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae908766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#in default pytorch do not require gradients\n",
    "x1 = torch.Tensor([2.0]).double()               ; x1.requires_grad= True\n",
    "x2 = torch.Tensor([0.0]).double()               ; x2.requires_grad= True\n",
    "w1 = torch.Tensor([-3.0]).double()              ; w1.requires_grad= True\n",
    "w2 = torch.Tensor([-1.0]).double()              ; w2.requires_grad= True\n",
    "b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad= True\n",
    "n=x1*w1 + x2*w2 + b\n",
    "o= torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print(\"---\")\n",
    "print(\"x2\",x2.grad.item())\n",
    "print(\"w2\",w2.grad.item())\n",
    "print(\"x1\",x1.grad.item())\n",
    "print(\"w1\",w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c4e3",
   "metadata": {},
   "source": [
    "## Build out a neural network library (multi-layer preceptron) in micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    #nin is num of input\n",
    "    def __init__(self, nin):\n",
    "        # weight\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        #bias\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "    \n",
    "    #x is input\n",
    "    def __call__(self, x):\n",
    "        #w*x+b\n",
    "        #zip takes two iterators and it creates a new iterator that iterates over the tuples of the corressponding entries\n",
    "        #activate function (forward)\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out=act.tanh()\n",
    "\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w +[self.b]\n",
    "\n",
    "class Layer:\n",
    "    #nout is number of output\n",
    "    def __init__(self,nin,nout):\n",
    "        self.neurons=[Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs=[n(x) for n in self.neurons]\n",
    "        return outs[0] if len (outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "        \n",
    "'''\n",
    "Multilayer Perceptrons (MLP) are the classical type of neural network.\n",
    "They are comprised of one or more layers of neurons. Data is fed to the input layer, \n",
    "there may be one or more hidden layers providing levels of abstraction, \n",
    "and predictions are made on the output layer, also called the visible layer.\n",
    "'''\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4c942",
   "metadata": {},
   "source": [
    "![neuron](https://github.com/soonkienyuan/NOTE-build-from-scratch-neural-networks-Micrograd/blob/main/photo/Dense-Neural-Network.png?raw=true=10x10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[2.0, 3.0,-1.0] # 3 dimentional input\n",
    "n=MLP(3,[4,4,1]) # same as image above\n",
    "'''\n",
    "3 input layer, 2 hidden layer of 4 and 1 output\n",
    "'''\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae91bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97743885",
   "metadata": {},
   "source": [
    "## creating a tiny dataset, writing the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 possible input in neural network\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0], # fed into neuron net to achive desired output (1.0)\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "\n",
    "# 4 desired targets  (desired output)\n",
    "ys = [1.0, -1.0, -1.0, 1.0] \n",
    "\n",
    "# '''\n",
    "# we want to nueron net have this output (desired target) (1.0)  \n",
    "# when fed this input [2.0, 3.0,-1.0] .....\n",
    "\n",
    "# same for other 3 \n",
    "\n",
    "# '''\n",
    "\n",
    "ypred=[n(x) for x in xs]\n",
    "ypred # get the predition. \n",
    "\n",
    "\n",
    "# for an example, the 1st prediction output is 0.93,\n",
    "# but we would like the output answer will be 1,\n",
    "# then we should put this higher\n",
    "\n",
    "# The 2nd prediction output is -0.37,\n",
    "# we want this output to become -1\n",
    "\n",
    "# The 3rd prediction output is -0.46,\n",
    "# we want this output becoe -1.0\n",
    "  \n",
    "# the 4th prediction output is -0.79\n",
    "# we want this output to become 1\n",
    "\n",
    "# so how do we make the neuron net and\n",
    "# tune the weights to better predict the desired targets \n",
    "\n",
    "# The trick used in DP is to achieve this is to \n",
    "# calculate a single number that measure the total performance of the \n",
    "# neuron net. \n",
    "\n",
    "# We called it the loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we intuitive sense that the neuron net is not performing well\n",
    "becoz the predicted is not close to the desired target\n",
    "\n",
    "Then the loss will be high and\n",
    "we want to minimize the loss\n",
    "\n",
    "we going to implement the mean square error loss\n",
    "\n",
    "and we need to minimize the loss\n",
    "\"\"\"\n",
    "#ygt is y ground truth \n",
    "#yout is y output\n",
    "loss=Value(0.0)\n",
    "for element in [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]:\n",
    "    loss=loss+ element\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb2979",
   "metadata": {},
   "source": [
    "## collecting the all parameters of the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.parameters() # this mlp have 41 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f00779",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n.parameters()) # this mlp have 41 parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe05885",
   "metadata": {},
   "source": [
    "## doing the gradient descent optimization manually tranning the network\n",
    "\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "We are thinking of the gradient as a vector pointing in the direction of increase loss\n",
    "\n",
    "In gradient descent, we modify the data by small step size in the direction of gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d20ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20dd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61a4a3",
   "metadata": {},
   "source": [
    "Based the above, the gradient is negative\n",
    "\n",
    "if the data goes lower, it will increase the loss.\n",
    "\n",
    "This is becoz the derivative of this neuron is negative, increasing the data will make the loss goes down.\n",
    "\n",
    "and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b031bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4960fa2",
   "metadata": {},
   "source": [
    "for an example, the value from 0.4152 increase to 0.465.\n",
    "\n",
    "This is good things , this is becoz slightly increase the data will make the loss goes down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee77e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss before gradient descent\n",
    "#let we compare the loss\n",
    "\n",
    "loss=Value(0.0)\n",
    "for element in [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]:\n",
    "    loss=loss+ element\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67910a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the loss after gradient descent\n",
    "ypred=[n(x) for x in xs]\n",
    "loss=Value(0.0)\n",
    "for element in [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]:\n",
    "    loss=loss+ element\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab195571",
   "metadata": {},
   "source": [
    "### And we repeat and repeat to increase the data by a small step and then the loss will be less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this the forward pass\n",
    "ypred=[n(x) for x in xs]\n",
    "loss=Value(0.0)\n",
    "for element in [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]:\n",
    "    loss=loss+ element\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab606401",
   "metadata": {},
   "source": [
    "This is just gradient descent, we just iteratively doing the forward pass and backward pass.\n",
    "\n",
    "Then the nueral network is improving its prediction.\n",
    "\n",
    "When we repeat increase or decrease the data based on the gradient, the loss will be less and the output will closely to the desired output.\n",
    "\n",
    "Take note we need to increase or decrease the data by a small step size is  to prevent the loss blow up.\n",
    "\n",
    "This is becoz we dont know how structure the loss look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1899de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d4ae6",
   "metadata": {},
   "source": [
    "## To make respectable and implement the actual tranning loop   \n",
    "\n",
    "Let re-initialize the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[2.0, 3.0,-1.0] # 3 dimentional input\n",
    "n=MLP(3,[4,4,1]) # same as image above\n",
    "'''\n",
    "3 input layer, 2 hidden layer of 4 and 1 output\n",
    "'''\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0], # fed into neuron net to achive desired output (1.0)\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "\n",
    "# 4 desired targets  (desired output)\n",
    "ys = [1.0, -1.0, -1.0, 1.0] \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b33e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=[n(x) for x in xs]\n",
    "ypred # get the predition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "for k in range(30):\n",
    "  \n",
    "  # forward pass\n",
    "  ypred = [n(x) for x in xs]\n",
    "  loss=Value(0.0)\n",
    "  for element in [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]:\n",
    "    loss=loss+ element\n",
    "  \n",
    "  # backward pass\n",
    "  for p in n.parameters():\n",
    "    # make sure that reset the grad=0\n",
    "    #then the actual backward pass accumulates the\n",
    "    #loss derivatives into the grads \n",
    "    p.grad = 0.0 \n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  for p in n.parameters():\n",
    "    p.data += -0.05 * p.grad\n",
    "  \n",
    "  print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23894d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de79d8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Neural network are the simple mathematics expression in the case of multi-layer perceptron that take input as the data and weight\n",
    "\n",
    "The parameters of the neural network mathematics expression for the forward pass followed by a loss function.\n",
    "\n",
    "The loss function try to measure the accuracy of the predictions and usually the loss will be low as your prediction matching your target or the network behaving well\n",
    "\n",
    "We manipulate the loss function so that when the loss function is low, the network is doing what you want it to do on your problem\n",
    "\n",
    "We backward the loss and use backpropagation to get the gradient and then we know how to tune all the parameters to decrease the loss locally.\n",
    "Then we need to iterate the process many times in what's  called the gradient descent\n",
    "\n",
    "Minimize the loss based on the gradient information.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "32ce2bf5b22447a9f16b8a05f945e6e7ffd9cddde243586790008dcb12882a9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
